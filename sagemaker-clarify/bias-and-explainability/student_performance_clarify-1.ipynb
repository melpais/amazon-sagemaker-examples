{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Bias and Explainability with Amazon SageMaker Clarify\n",
    "\n",
    "## Overview\n",
    "Biases are imbalances in the training data, or the prediction behavior of the model across different groups. Sometimes these biases can cause harms to demographic subgroups, e.g. based age or income bracket. The field of machine learning provides an opportunity to address biases by detecting them and measuring them in your data and model.\n",
    "\n",
    "Amazon SageMaker Clarify provides machine learning developers with greater visibility into their training data and models so they can identify and limit bias and explain predictions.\n",
    "\n",
    "In this notebook, we are going to go through each stage of the ML lifecycle, and show where you can include Clarify.\n",
    "\n",
    "## Problem Formation\n",
    "\n",
    "In this notebook, we are looking to predict the final grade for a students in a maths class, from the popular [Student Performance dataset](https://archive.ics.uci.edu/ml/datasets/Student+Performance) courtesy of UC Irvine.\n",
    "\n",
    "For this dataset, final grades range from 0-20, where 15-20 are the most favourable outcomes. This is a multiclass classification problem, where we want to predict which grade a given student will get from 0 to 20. \n",
    "\n",
    "The benefit of using ML to predict this, is to be able to provide an accurate grade for the student if they aren't able to attend the final exam, due to circumstances outside their control.\n",
    "\n",
    "#TODO diagram\n",
    "\n",
    "The notebook will take 90 minutes to execute and will cost approximately $2. TODO\n",
    " - Some estimate of both time and money is recommended.\n",
    "    - List the instance types and other resources that are created.\n",
    "\n",
    "## Prerequisites\n",
    "1. This notebook works in the following environments.\n",
    "   - Notebook Instances: Jupyter\n",
    "   - Notebook Instances: JupyterLab\n",
    "   - Studio\n",
    "1. Which conda kernel is required? TODO\n",
    "1. This is a standalone notebook and it does not depend on other notebooks.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup \n",
    "\n",
    "### Setup Dependencies\n",
    "\n",
    "First, we're going to import various python libraries and set up a Sagemaker session for various tasks throughout our notebook. Then, we'll create a prefix within the default sagemaker bucket to store our data and reports."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# imports\n",
    "import sagemaker\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "# initialisation\n",
    "session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = session.default_bucket()\n",
    "prefix = \"sagemaker/student-data-xgb\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup Python Modules\n",
    "1. Import modules, set options, and activate extensions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# imports\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# options\n",
    "pd.options.display.max_columns = 50\n",
    "pd.options.display.max_rows = 30\n",
    "\n",
    "# extensions\n",
    "if 'autoreload' not in get_ipython().extension_manager.loaded:\n",
    "    %load_ext autoreload\n",
    "    \n",
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T14:44:50.874881Z",
     "start_time": "2019-06-16T14:44:38.616867Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parameters\n",
    "1. Setup user supplied parameters like custom bucket names and roles in a separated cell and call out what their options are.\n",
    "1. Use defaults, so the notebook will still run end-to-end without any user modification.\n",
    "\n",
    "For example, the following description & code block prompts the user to select the preferred dataset.\n",
    "\n",
    "~~~\n",
    "\n",
    "To do select a particular dataset, assign choosen_data_set below to be one of 'diabetes', 'california', or 'boston' where each name corresponds to the it's respective dataset.\n",
    "\n",
    "'boston' : boston house data\n",
    "'california' : california house data\n",
    "'diabetes' : diabetes data\n",
    "\n",
    "~~~\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_sets = {\n",
    "    \"diabetes\": \"load_diabetes()\",\n",
    "    \"california\": \"fetch_california_housing()\",\n",
    "    \"boston\": \"load_boston()\",\n",
    "}\n",
    "\n",
    "# Change choosen_data_set variable to one of the data sets above.\n",
    "choosen_data_set = \"california\"\n",
    "assert choosen_data_set in data_sets.keys()\n",
    "print(\"I selected the '{}' dataset!\".format(choosen_data_set))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Data import\n",
    "1. Look for the data that was stored by a previous notebook run `%store -r variableName`\n",
    "1. If that doesn't exist, look in S3 in their default bucket\n",
    "1. If that doesn't exist, download it from the [SageMaker dataset bucket](https://sagemaker-sample-files.s3.amazonaws.com/) \n",
    "1. If that doesn't exist, download it from origin\n",
    "\n",
    "For example, the following code block will pull training and validation data that was created in a previous notebook. This allows the customer to experiment with features, re-run the notebook, and not have it pull the dataset over and over."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load relevant dataframes and variables from preprocessing_tabular_data.ipynb required for this notebook\n",
    "%store -r X_train\n",
    "%store -r X_test\n",
    "%store -r X_val\n",
    "%store -r Y_train\n",
    "%store -r Y_test\n",
    "%store -r Y_val\n",
    "%store -r choosen_data_set"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Procedure or tutorial\n",
    "1. Break up processes with Markdown blocks to explain what's going on.\n",
    "1. Make use of visualizations to better demonstrate each step."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cleanup\n",
    "You can keep your endpoint running to continue capturing data. If you do not plan to collect more data or use this endpoint further, you should delete the endpoint to avoid incurring additional charges. Note that deleting your endpoint does not delete the data that was captured during the model invocations. That data persists in Amazon S3 until you delete it yourself.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "session.delete_endpoint(endpoint_name)\n",
    "session.delete_model(pipeline_model.name)\n",
    "\n",
    "# Clean up S3 model? TODO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Next steps\n",
    "\n",
    "AI services and machine learning are helping organisations to build data driven applications that are innovative and can be highly attuned to their customersâ€™ needs, but AI applications require crucial customer data to train machine learning models. Application logic is delegated to these models, which can introduce unfairness and biases into an application. In this session we reviewed the machine learning techniques and AWS services you can use to understand and reduce these risks.\n",
    "\n",
    "#TODO"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "1. Pauline Kelly - [Building AI applications that avoid bias and maintain privacy and fairness](https://anz-resources.awscloud.com/aws-summit-online-anz-2021-data-scientist/building-ai-applications-that-avoid-bias-and-maintain-privacy-and-fairness-1)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('.venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "interpreter": {
   "hash": "70ebf011cb0c42bf12e9cdb846e12ab7aa90f34e84f2441f1ae1cb0d1fcddbd4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}